# general
gpu_id: 0
use_gpu: True
seed: [999]

# Memory optimization settings
use_amp: True                # Mixed Precision Training (FP16) - reduces memory by ~40-50%
keep_features_on_cpu: True   # Keep frozen embeddings on CPU - saves ~450MB GPU memory
use_checkpoint: True         # Gradient checkpointing - trades compute for ~20-30% memory reduction

# Alternative Acceleration Strategies (safer for large datasets)
use_torch_compile: True         # Enable torch.compile for ~20-30% speedup (PyTorch 2.0+)
clip_grad_norm: {'max_norm': 5.0, 'norm_type': 2}         # Gradient clipping to prevent NaN (optional, helps with numerical stability)

# multi-modal raw features
data_path: '../data/'
inter_splitting_label: 'x_label'
filter_out_cod_start_users: True
is_multimodal_model: True

checkpoint_dir: 'saved'
save_recommended_topk: True
recommend_topk: 'recommend_topk/'

embedding_size: 64
weight_decay: 0.0
req_training: True
#embedding_size: 3780

# training settings
epochs: 200
stopping_step: 20
train_batch_size: 4096  # Increased from 2048 for faster training (2x speedup)
learner: adam
learning_rate: 0.001
learning_rate_scheduler: [1.0, 50]
eval_step: 1

training_neg_sample_num: 1
use_neg_sampling: True
use_full_sampling: False
NEG_PREFIX: neg__

USER_ID_FIELD: user_id:token
ITEM_ID_FIELD: item_id:token
TIME_FIELD: timestamp:float
field_separator: "\t"


# evaluation settings
metrics: ["Recall", "NDCG", "Precision", "MAP"]
topk: [20]      # [5, 10, 20, 50]
valid_metric: Recall@20
eval_batch_size: 4096

# Candidate sampling for evaluation (improves interpretability for cold-start)
use_candidate_sampling: True        # Enable candidate sampling during evaluation
candidate_sampling_type: 'uniform'  # Options: 'uniform', 'popularity', 'both', 'none'
                                    # uniform: randomly sample negative items
                                    # popularity: sample negatives based on item popularity
                                    # both: report both uniform and popularity-based metrics
                                    # none: full-rank evaluation (same as use_candidate_sampling: False)
num_negative_candidates: 99         # Number of negative items to sample per positive
                                    # Typical ratios: 99 (1:99), 199 (1:199), 999 (1:999)

#
use_raw_features: False
max_txt_len: 32
max_img_size: 256
vocab_size: 30522
type_vocab_size: 2
hidden_size: 4
pad_token_id: 0
max_position_embeddings: 512
layer_norm_eps: 1e-12
hidden_dropout_prob: 0.1

end2end: False

# ============================================================================
# FETTLE Multi-Modal Alignment Settings
# Paper: "Who To Align With: Feedback-Oriented Multi-Modal Alignment 
#         in Recommendation Systems" (SIGIR 2024)
# ============================================================================
use_fettle: False  # Feature flag to enable/disable FETTLE
cf_extraction_method: 'average'  # Only average method supported

# FETTLE Loss Weights
iladt_weight: 0.05       # Weight for ILA+DT loss
cla_weight: 0.01         # Weight for CLA loss

# FETTLE Hyperparameters
clcr_gamma: 0.05         # Temperature for ILA+DT (>= 0.01 to prevent NaN)
ga_gamma: 0.1            # Temperature for CLA
prototype_num: 10        # Number of prototypes/clusters for CLA

# iteration parameters
hyper_parameters: ["seed"]
