# general
gpu_id: 0
use_gpu: True
seed: [999]

# Mixed Precision Training (FP16) - DISABLED for MMGCN due to numerical instability
# MMGCN's loss function (log + sigmoid + matmul) is prone to NaN in FP16
# Use alternative acceleration strategies below instead
use_amp: False

# Alternative Acceleration Strategies (safer for large datasets)
# 1. Larger batch size (most effective for speed)
# 2. DataLoader workers for parallel data loading
# 3. Gradient accumulation for effective larger batches
# 4. PyTorch 2.0+ compilation (torch.compile)
num_workers: 4  # Parallel data loading (adjust based on CPU cores)
use_torch_compile: True  # Enable torch.compile for ~20-30% speedup (PyTorch 2.0+)

# Gradient clipping to prevent NaN (optional, helps with numerical stability)
# Set max_norm to clip gradients, e.g., {'max_norm': 1.0, 'norm_type': 2}
clip_grad_norm: {'max_norm': 5.0, 'norm_type': 2}

# multi-modal raw features
data_path: '../data/'
inter_splitting_label: 'x_label'
filter_out_cod_start_users: True
is_multimodal_model: True

checkpoint_dir: 'saved'
save_recommended_topk: True
recommend_topk: 'recommend_topk/'

embedding_size: 64
weight_decay: 0.0
req_training: True
#embedding_size: 3780

# training settings
epochs: 200
stopping_step: 20
train_batch_size: 4096  # Increased from 2048 for faster training (2x speedup)
learner: adam
learning_rate: 0.001
learning_rate_scheduler: [1.0, 50]
eval_step: 1

training_neg_sample_num: 1
use_neg_sampling: True
use_full_sampling: False
NEG_PREFIX: neg__

USER_ID_FIELD: user_id:token
ITEM_ID_FIELD: item_id:token
TIME_FIELD: timestamp:float
field_separator: "\t"


# evaluation settings
metrics: ["Recall", "NDCG", "Precision", "MAP"]
topk: [20]      # [5, 10, 20, 50]
valid_metric: Recall@20
eval_batch_size: 4096

#
use_raw_features: False
max_txt_len: 32
max_img_size: 256
vocab_size: 30522
type_vocab_size: 2
hidden_size: 4
pad_token_id: 0
max_position_embeddings: 512
layer_norm_eps: 1e-12
hidden_dropout_prob: 0.1

end2end: False

# iteration parameters
hyper_parameters: ["seed"]
